{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /home/aqumar/Desktop/hippocampal_volume_segmentation/venv/lib/python3.6/site-packages (3.3.4)\r\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/aqumar/Desktop/hippocampal_volume_segmentation/venv/lib/python3.6/site-packages (from matplotlib) (2.8.1)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/aqumar/Desktop/hippocampal_volume_segmentation/venv/lib/python3.6/site-packages (from matplotlib) (1.3.1)\r\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /home/aqumar/Desktop/hippocampal_volume_segmentation/venv/lib/python3.6/site-packages (from matplotlib) (2.4.7)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/aqumar/Desktop/hippocampal_volume_segmentation/venv/lib/python3.6/site-packages (from matplotlib) (8.3.1)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /home/aqumar/Desktop/hippocampal_volume_segmentation/venv/lib/python3.6/site-packages (from matplotlib) (0.10.0)\r\n",
      "Requirement already satisfied: numpy>=1.15 in /home/aqumar/Desktop/hippocampal_volume_segmentation/venv/lib/python3.6/site-packages (from matplotlib) (1.19.5)\r\n",
      "Requirement already satisfied: six in /home/aqumar/Desktop/hippocampal_volume_segmentation/venv/lib/python3.6/site-packages (from cycler>=0.10->matplotlib) (1.16.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "  Using cached torchvision-0.10.0-cp36-cp36m-manylinux1_x86_64.whl (22.1 MB)\n",
      "Collecting torch==1.9.0\n",
      "  Downloading torch-1.9.0-cp36-cp36m-manylinux1_x86_64.whl (831.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 831.4 MB 33 kB/s  eta 0:00:01     |███████████████████████▌        | 610.3 MB 409 kB/s eta 0:09:01     |█████████████████████████████▍  | 764.2 MB 556 kB/s eta 0:02:01\n",
      "\u001b[?25hRequirement already satisfied: pillow>=5.3.0 in /home/aqumar/Desktop/hippocampal_volume_segmentation/venv/lib/python3.6/site-packages (from torchvision) (8.3.1)\n",
      "Requirement already satisfied: numpy in /home/aqumar/Desktop/hippocampal_volume_segmentation/venv/lib/python3.6/site-packages (from torchvision) (1.19.5)\n",
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='pypi.org', port=443): Read timed out. (read timeout=15)\",)': /simple/dataclasses/\u001b[0m\n",
      "Collecting dataclasses\n",
      "  Using cached dataclasses-0.8-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: typing-extensions in /home/aqumar/Desktop/hippocampal_volume_segmentation/venv/lib/python3.6/site-packages (from torch==1.9.0->torchvision) (3.10.0.0)\n",
      "Installing collected packages: dataclasses, torch, torchvision\n",
      "Successfully installed dataclasses-0.8 torch-1.9.0 torchvision-0.10.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorboard\n",
      "  Using cached tensorboard-2.5.0-py3-none-any.whl (6.0 MB)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.33.0-py2.py3-none-any.whl (151 kB)\n",
      "\u001b[K     |████████████████████████████████| 151 kB 340 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /home/aqumar/Desktop/hippocampal_volume_segmentation/venv/lib/python3.6/site-packages (from tensorboard) (57.1.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/aqumar/Desktop/hippocampal_volume_segmentation/venv/lib/python3.6/site-packages (from tensorboard) (0.36.2)\n",
      "Collecting absl-py>=0.4\n",
      "  Downloading absl_py-0.13.0-py3-none-any.whl (132 kB)\n",
      "\u001b[K     |████████████████████████████████| 132 kB 1.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.12.0 in /home/aqumar/Desktop/hippocampal_volume_segmentation/venv/lib/python3.6/site-packages (from tensorboard) (1.19.5)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.4-py2.py3-none-any.whl (18 kB)\n",
      "Collecting protobuf>=3.6.0\n",
      "  Downloading protobuf-3.17.3-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 194 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio>=1.24.3\n",
      "  Downloading grpcio-1.38.1-cp36-cp36m-manylinux2014_x86_64.whl (4.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.2 MB 167 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests<3,>=2.21.0\n",
      "  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
      "\u001b[K     |████████████████████████████████| 62 kB 483 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-2.0.1-py3-none-any.whl (288 kB)\n",
      "\u001b[K     |████████████████████████████████| 288 kB 885 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "Requirement already satisfied: six in /home/aqumar/Desktop/hippocampal_volume_segmentation/venv/lib/python3.6/site-packages (from absl-py>=0.4->tensorboard) (1.16.0)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Using cached cachetools-4.2.2-py3-none-any.whl (11 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata in /home/aqumar/Desktop/hippocampal_volume_segmentation/venv/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard) (4.6.1)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting charset-normalizer~=2.0.0\n",
      "  Downloading charset_normalizer-2.0.2-py3-none-any.whl (35 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.2-py3-none-any.whl (59 kB)\n",
      "\u001b[K     |████████████████████████████████| 59 kB 1.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.6-py2.py3-none-any.whl (138 kB)\n",
      "\u001b[K     |████████████████████████████████| 138 kB 1.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting certifi>=2017.4.17\n",
      "  Downloading certifi-2021.5.30-py2.py3-none-any.whl (145 kB)\n",
      "\u001b[K     |████████████████████████████████| 145 kB 1.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\n",
      "\u001b[K     |████████████████████████████████| 146 kB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: dataclasses in /home/aqumar/Desktop/hippocampal_volume_segmentation/venv/lib/python3.6/site-packages (from werkzeug>=0.11.15->tensorboard) (0.8)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/aqumar/Desktop/hippocampal_volume_segmentation/venv/lib/python3.6/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard) (3.10.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/aqumar/Desktop/hippocampal_volume_segmentation/venv/lib/python3.6/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard) (3.5.0)\n",
      "Installing collected packages: urllib3, pyasn1, idna, charset-normalizer, certifi, rsa, requests, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, google-auth, werkzeug, tensorboard-plugin-wit, tensorboard-data-server, protobuf, markdown, grpcio, google-auth-oauthlib, absl-py, tensorboard\n",
      "Successfully installed absl-py-0.13.0 cachetools-4.2.2 certifi-2021.5.30 charset-normalizer-2.0.2 google-auth-1.33.0 google-auth-oauthlib-0.4.4 grpcio-1.38.1 idna-3.2 markdown-3.3.4 oauthlib-3.1.1 protobuf-3.17.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-2.26.0 requests-oauthlib-1.3.0 rsa-4.7.2 tensorboard-2.5.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 urllib3-1.26.6 werkzeug-2.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchsummary\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n",
      "Installing collected packages: torchsummary\n",
      "Successfully installed torchsummary-1.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting medpy\n",
      "  Downloading MedPy-0.4.0.tar.gz (151 kB)\n",
      "\u001b[K     |████████████████████████████████| 151 kB 2.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scipy>=1.1.0\n",
      "  Using cached scipy-1.5.4-cp36-cp36m-manylinux1_x86_64.whl (25.9 MB)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /home/aqumar/Desktop/hippocampal_volume_segmentation/venv/lib/python3.6/site-packages (from medpy) (1.19.5)\n",
      "Collecting SimpleITK>=1.1.0\n",
      "  Downloading SimpleITK-2.0.2-cp36-cp36m-manylinux2010_x86_64.whl (47.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 47.4 MB 575 kB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: medpy\n",
      "  Building wheel for medpy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for medpy: filename=MedPy-0.4.0-py3-none-any.whl size=214963 sha256=ed3902258368e5d89dd2e0e010296fb7df8ad014a67ee301aee08c6ee6c5ef77\n",
      "  Stored in directory: /home/aqumar/.cache/pip/wheels/62/5d/88/2816a43ab870de19b56a48413dcd835495dcbab36370fb9020\n",
      "Successfully built medpy\n",
      "Installing collected packages: SimpleITK, scipy, medpy\n",
      "Successfully installed SimpleITK-2.0.2 medpy-0.4.0 scipy-1.5.4\n"
     ]
    }
   ],
   "source": [
    "!pip install medpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0-py2.py3-none-any.whl\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-0.24.2-cp36-cp36m-manylinux2010_x86_64.whl (22.2 MB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-2.2.0-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/aqumar/Desktop/hippocampal_volume_segmentation/venv/lib/python3.6/site-packages (from scikit-learn->sklearn) (1.5.4)\n",
      "Collecting joblib>=0.11\n",
      "  Using cached joblib-1.0.1-py3-none-any.whl (303 kB)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/aqumar/Desktop/hippocampal_volume_segmentation/venv/lib/python3.6/site-packages (from scikit-learn->sklearn) (1.19.5)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn, sklearn\n",
      "Successfully installed joblib-1.0.1 scikit-learn-0.24.2 sklearn-0.0 threadpoolctl-2.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This file contains code that will kick off training and testing processes\n",
    "\"\"\"\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from experiments.UNetExperiment import UNetExperiment\n",
    "from data_prep.HippocampusDatasetLoader import LoadHippocampusData\n",
    "\n",
    "class Config:\n",
    "    \"\"\"\n",
    "    Holds configuration parameters\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Defines important parameters:\n",
    "        name: name of algorithm (neural network)\n",
    "        root_dir: \n",
    "        n_epochs: number of epochs to train\n",
    "        learning_rate: initial learning rate\n",
    "        batch_size: size of image/label batches to feed the neural network at training at once\n",
    "        patch_size:\n",
    "        test_results_dir: \n",
    "        \"\"\"\n",
    "        self.name = 'Basic_unet'\n",
    "        # self.root_dir = r'../data/TrainingSet/'\n",
    "        self.root_dir = r'../../section1/data/TrainingSet/'\n",
    "        self.n_epochs = 1\n",
    "        self.learning_rate = 0.0002\n",
    "        self.batch_size = 16\n",
    "        self.patch_size = 64\n",
    "        self.test_results_dir = '../results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed generator:\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "c = Config()\n",
    "\n",
    "# Load data\n",
    "\n",
    "data = LoadHippocampusData(root_dir=c.root_dir, y_shape=c.patch_size,\n",
    "                               z_shape=c.patch_size)\n",
    "\n",
    "keys = range(len(data))\n",
    "keys = np.array(keys)\n",
    "np.random.shuffle(keys)\n",
    "\n",
    "split = dict()\n",
    "\n",
    "\n",
    "train_val, split['test'] = train_test_split(keys, test_size = 0.15)\n",
    "split['train'], split['val'] = train_test_split(train_val, test_size = 0.15)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNet(\n",
      "  (model): UnetSkipConnectionBlock(\n",
      "    (model): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      )\n",
      "      (2): UnetSkipConnectionBlock(\n",
      "        (model): Sequential(\n",
      "          (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "          (1): Sequential(\n",
      "            (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "            (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "          (2): Sequential(\n",
      "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "            (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "          (3): UnetSkipConnectionBlock(\n",
      "            (model): Sequential(\n",
      "              (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "              (1): Sequential(\n",
      "                (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "                (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "              )\n",
      "              (2): Sequential(\n",
      "                (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "                (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "              )\n",
      "              (3): UnetSkipConnectionBlock(\n",
      "                (model): Sequential(\n",
      "                  (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "                  (1): Sequential(\n",
      "                    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                    (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "                    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "                  )\n",
      "                  (2): Sequential(\n",
      "                    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                    (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "                    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "                  )\n",
      "                  (3): UnetSkipConnectionBlock(\n",
      "                    (model): Sequential(\n",
      "                      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "                      (1): Sequential(\n",
      "                        (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                        (1): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "                        (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "                      )\n",
      "                      (2): Sequential(\n",
      "                        (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                        (1): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "                        (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "                      )\n",
      "                      (3): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
      "                    )\n",
      "                  )\n",
      "                  (4): Sequential(\n",
      "                    (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                    (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "                  )\n",
      "                  (5): Sequential(\n",
      "                    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                    (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "                  )\n",
      "                  (6): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "                )\n",
      "              )\n",
      "              (4): Sequential(\n",
      "                (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "              )\n",
      "              (5): Sequential(\n",
      "                (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "              )\n",
      "              (6): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
      "            )\n",
      "          )\n",
      "          (4): Sequential(\n",
      "            (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "          (5): Sequential(\n",
      "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "          )\n",
      "          (6): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "        )\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      )\n",
      "      (4): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "      )\n",
      "      (5): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 64, 64]             640\n",
      "    InstanceNorm2d-2           [-1, 64, 64, 64]               0\n",
      "         LeakyReLU-3           [-1, 64, 64, 64]               0\n",
      "            Conv2d-4           [-1, 64, 64, 64]          36,928\n",
      "    InstanceNorm2d-5           [-1, 64, 64, 64]               0\n",
      "         LeakyReLU-6           [-1, 64, 64, 64]               0\n",
      "         MaxPool2d-7           [-1, 64, 32, 32]               0\n",
      "            Conv2d-8          [-1, 128, 32, 32]          73,856\n",
      "    InstanceNorm2d-9          [-1, 128, 32, 32]               0\n",
      "        LeakyReLU-10          [-1, 128, 32, 32]               0\n",
      "           Conv2d-11          [-1, 128, 32, 32]         147,584\n",
      "   InstanceNorm2d-12          [-1, 128, 32, 32]               0\n",
      "        LeakyReLU-13          [-1, 128, 32, 32]               0\n",
      "        MaxPool2d-14          [-1, 128, 16, 16]               0\n",
      "           Conv2d-15          [-1, 256, 16, 16]         295,168\n",
      "   InstanceNorm2d-16          [-1, 256, 16, 16]               0\n",
      "        LeakyReLU-17          [-1, 256, 16, 16]               0\n",
      "           Conv2d-18          [-1, 256, 16, 16]         590,080\n",
      "   InstanceNorm2d-19          [-1, 256, 16, 16]               0\n",
      "        LeakyReLU-20          [-1, 256, 16, 16]               0\n",
      "        MaxPool2d-21            [-1, 256, 8, 8]               0\n",
      "           Conv2d-22            [-1, 512, 8, 8]       1,180,160\n",
      "   InstanceNorm2d-23            [-1, 512, 8, 8]               0\n",
      "        LeakyReLU-24            [-1, 512, 8, 8]               0\n",
      "           Conv2d-25            [-1, 512, 8, 8]       2,359,808\n",
      "   InstanceNorm2d-26            [-1, 512, 8, 8]               0\n",
      "        LeakyReLU-27            [-1, 512, 8, 8]               0\n",
      "        MaxPool2d-28            [-1, 512, 4, 4]               0\n",
      "           Conv2d-29           [-1, 1024, 4, 4]       4,719,616\n",
      "   InstanceNorm2d-30           [-1, 1024, 4, 4]               0\n",
      "        LeakyReLU-31           [-1, 1024, 4, 4]               0\n",
      "           Conv2d-32           [-1, 1024, 4, 4]       9,438,208\n",
      "   InstanceNorm2d-33           [-1, 1024, 4, 4]               0\n",
      "        LeakyReLU-34           [-1, 1024, 4, 4]               0\n",
      "  ConvTranspose2d-35            [-1, 512, 8, 8]       2,097,664\n",
      "UnetSkipConnectionBlock-36           [-1, 1024, 8, 8]               0\n",
      "           Conv2d-37            [-1, 512, 8, 8]       4,719,104\n",
      "        LeakyReLU-38            [-1, 512, 8, 8]               0\n",
      "           Conv2d-39            [-1, 512, 8, 8]       2,359,808\n",
      "        LeakyReLU-40            [-1, 512, 8, 8]               0\n",
      "  ConvTranspose2d-41          [-1, 256, 16, 16]         524,544\n",
      "UnetSkipConnectionBlock-42          [-1, 512, 16, 16]               0\n",
      "           Conv2d-43          [-1, 256, 16, 16]       1,179,904\n",
      "        LeakyReLU-44          [-1, 256, 16, 16]               0\n",
      "           Conv2d-45          [-1, 256, 16, 16]         590,080\n",
      "        LeakyReLU-46          [-1, 256, 16, 16]               0\n",
      "  ConvTranspose2d-47          [-1, 128, 32, 32]         131,200\n",
      "UnetSkipConnectionBlock-48          [-1, 256, 32, 32]               0\n",
      "           Conv2d-49          [-1, 128, 32, 32]         295,040\n",
      "        LeakyReLU-50          [-1, 128, 32, 32]               0\n",
      "           Conv2d-51          [-1, 128, 32, 32]         147,584\n",
      "        LeakyReLU-52          [-1, 128, 32, 32]               0\n",
      "  ConvTranspose2d-53           [-1, 64, 64, 64]          32,832\n",
      "UnetSkipConnectionBlock-54          [-1, 128, 64, 64]               0\n",
      "           Conv2d-55           [-1, 64, 64, 64]          73,792\n",
      "        LeakyReLU-56           [-1, 64, 64, 64]               0\n",
      "           Conv2d-57           [-1, 64, 64, 64]          36,928\n",
      "        LeakyReLU-58           [-1, 64, 64, 64]               0\n",
      "           Conv2d-59            [-1, 3, 64, 64]             195\n",
      "UnetSkipConnectionBlock-60            [-1, 3, 64, 64]               0\n",
      "================================================================\n",
      "Total params: 31,030,723\n",
      "Trainable params: 31,030,723\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 50.62\n",
      "Params size (MB): 118.37\n",
      "Estimated Total Size (MB): 169.01\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "exp = UNetExperiment(config=c, split=split, dataset=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment started.\n",
      "Training epoch 0...\n",
      "\n",
      "Epoch: 0 Train loss: 1.0566959381103516, 0.2% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.19029611349105835, 2.7% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.05989430844783783, 5.1% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.07803082466125488, 7.5% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.06714083254337311, 9.9% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.06494609266519547, 12.3% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.06356941163539886, 14.8% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.0448274090886116, 17.2% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.02481715939939022, 19.6% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.03891516476869583, 22.0% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.04462812840938568, 24.5% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.03587377443909645, 26.9% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.03458691015839577, 29.3% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.03412700816988945, 31.7% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.03582967072725296, 34.1% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.026930613443255424, 36.6% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.029732339084148407, 39.0% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.023963747546076775, 41.4% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.016494465991854668, 43.8% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.01769435591995716, 46.2% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.020475303754210472, 48.7% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.017191454768180847, 51.1% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.018887314945459366, 53.5% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.01530633307993412, 55.9% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.01957411877810955, 58.4% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.015798751264810562, 60.8% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.02350802905857563, 63.2% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.013179831206798553, 65.6% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.024687744677066803, 68.0% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.026041163131594658, 70.5% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.021641472354531288, 72.9% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.019909849390387535, 75.3% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.015221825800836086, 77.7% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.016155768185853958, 80.1% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.012455586344003677, 82.6% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.0177017692476511, 85.0% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.009995345026254654, 87.4% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.020833127200603485, 89.8% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.018705517053604126, 92.3% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.017490213736891747, 94.7% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.015957463532686234, 97.1% complete\n",
      "..........\n",
      "Epoch: 0 Train loss: 0.019194861873984337, 99.5% complete\n",
      "...\n",
      "Training complete\n",
      "Validating epoch 0...\n",
      "Batch 0. Data shape torch.Size([16, 1, 64, 64]) Loss 0.027476901188492775\n",
      "Batch 1. Data shape torch.Size([16, 1, 64, 64]) Loss 0.01585625484585762\n",
      "Batch 2. Data shape torch.Size([16, 1, 64, 64]) Loss 0.01608465425670147\n",
      "Batch 3. Data shape torch.Size([16, 1, 64, 64]) Loss 0.017184916883707047\n",
      "Batch 4. Data shape torch.Size([16, 1, 64, 64]) Loss 0.01491076685488224\n",
      "Batch 5. Data shape torch.Size([16, 1, 64, 64]) Loss 0.01927867718040943\n",
      "Batch 6. Data shape torch.Size([16, 1, 64, 64]) Loss 0.01272512972354889\n",
      "Batch 7. Data shape torch.Size([16, 1, 64, 64]) Loss 0.014576039277017117\n",
      "Batch 8. Data shape torch.Size([16, 1, 64, 64]) Loss 0.017818108201026917\n",
      "Batch 9. Data shape torch.Size([16, 1, 64, 64]) Loss 0.021269578486680984\n",
      "Batch 10. Data shape torch.Size([16, 1, 64, 64]) Loss 0.016908491030335426\n",
      "Batch 11. Data shape torch.Size([16, 1, 64, 64]) Loss 0.013315875083208084\n",
      "Batch 12. Data shape torch.Size([16, 1, 64, 64]) Loss 0.016725854948163033\n",
      "Batch 13. Data shape torch.Size([16, 1, 64, 64]) Loss 0.020702458918094635\n",
      "Batch 14. Data shape torch.Size([16, 1, 64, 64]) Loss 0.025512751191854477\n",
      "Batch 15. Data shape torch.Size([16, 1, 64, 64]) Loss 0.01701824553310871\n",
      "Batch 16. Data shape torch.Size([16, 1, 64, 64]) Loss 0.0128129031509161\n",
      "Batch 17. Data shape torch.Size([16, 1, 64, 64]) Loss 0.019600339233875275\n",
      "Batch 18. Data shape torch.Size([16, 1, 64, 64]) Loss 0.014106730930507183\n",
      "Batch 19. Data shape torch.Size([16, 1, 64, 64]) Loss 0.02090100385248661\n",
      "Batch 20. Data shape torch.Size([16, 1, 64, 64]) Loss 0.016578365117311478\n",
      "Batch 21. Data shape torch.Size([16, 1, 64, 64]) Loss 0.008566157892346382\n",
      "Batch 22. Data shape torch.Size([16, 1, 64, 64]) Loss 0.020919103175401688\n",
      "Batch 23. Data shape torch.Size([16, 1, 64, 64]) Loss 0.021012917160987854\n",
      "Batch 24. Data shape torch.Size([16, 1, 64, 64]) Loss 0.0242865439504385\n",
      "Batch 25. Data shape torch.Size([16, 1, 64, 64]) Loss 0.010932674631476402\n",
      "Batch 26. Data shape torch.Size([16, 1, 64, 64]) Loss 0.020906487479805946\n",
      "Batch 27. Data shape torch.Size([16, 1, 64, 64]) Loss 0.014073975384235382\n",
      "Batch 28. Data shape torch.Size([16, 1, 64, 64]) Loss 0.010242251679301262\n",
      "Batch 29. Data shape torch.Size([16, 1, 64, 64]) Loss 0.021703628823161125\n",
      "Batch 30. Data shape torch.Size([16, 1, 64, 64]) Loss 0.019483746960759163\n",
      "Batch 31. Data shape torch.Size([16, 1, 64, 64]) Loss 0.028152551501989365\n",
      "Batch 32. Data shape torch.Size([16, 1, 64, 64]) Loss 0.014834546484053135\n",
      "Batch 33. Data shape torch.Size([16, 1, 64, 64]) Loss 0.018427617847919464\n",
      "Batch 34. Data shape torch.Size([16, 1, 64, 64]) Loss 0.02013595961034298\n",
      "Batch 35. Data shape torch.Size([16, 1, 64, 64]) Loss 0.022863950580358505\n",
      "Batch 36. Data shape torch.Size([16, 1, 64, 64]) Loss 0.02308931015431881\n",
      "Batch 37. Data shape torch.Size([16, 1, 64, 64]) Loss 0.014111041091382504\n",
      "Batch 38. Data shape torch.Size([16, 1, 64, 64]) Loss 0.012672347947955132\n",
      "Batch 39. Data shape torch.Size([16, 1, 64, 64]) Loss 0.01549510471522808\n",
      "Batch 40. Data shape torch.Size([16, 1, 64, 64]) Loss 0.01763981766998768\n",
      "Batch 41. Data shape torch.Size([16, 1, 64, 64]) Loss 0.012202315963804722\n",
      "Batch 42. Data shape torch.Size([16, 1, 64, 64]) Loss 0.02755420282483101\n",
      "Batch 43. Data shape torch.Size([16, 1, 64, 64]) Loss 0.014843123033642769\n",
      "Batch 44. Data shape torch.Size([16, 1, 64, 64]) Loss 0.018667923286557198\n",
      "Batch 45. Data shape torch.Size([16, 1, 64, 64]) Loss 0.006375744938850403\n",
      "Batch 46. Data shape torch.Size([16, 1, 64, 64]) Loss 0.019086817279458046\n",
      "Batch 47. Data shape torch.Size([16, 1, 64, 64]) Loss 0.023395853117108345\n",
      "Batch 48. Data shape torch.Size([16, 1, 64, 64]) Loss 0.01443586964160204\n",
      "Batch 49. Data shape torch.Size([16, 1, 64, 64]) Loss 0.01959134265780449\n",
      "Batch 50. Data shape torch.Size([16, 1, 64, 64]) Loss 0.013312406837940216\n",
      "Batch 51. Data shape torch.Size([16, 1, 64, 64]) Loss 0.01093168556690216\n",
      "Batch 52. Data shape torch.Size([16, 1, 64, 64]) Loss 0.01858670637011528\n",
      "Batch 53. Data shape torch.Size([16, 1, 64, 64]) Loss 0.01867738366127014\n",
      "Batch 54. Data shape torch.Size([16, 1, 64, 64]) Loss 0.019581150263547897\n",
      "Batch 55. Data shape torch.Size([16, 1, 64, 64]) Loss 0.02829241380095482\n",
      "Batch 56. Data shape torch.Size([16, 1, 64, 64]) Loss 0.012601960450410843\n",
      "Batch 57. Data shape torch.Size([16, 1, 64, 64]) Loss 0.01382368803024292\n",
      "Batch 58. Data shape torch.Size([16, 1, 64, 64]) Loss 0.026049146428704262\n",
      "Batch 59. Data shape torch.Size([16, 1, 64, 64]) Loss 0.023404709994792938\n",
      "Batch 60. Data shape torch.Size([16, 1, 64, 64]) Loss 0.015260754153132439\n",
      "Batch 61. Data shape torch.Size([16, 1, 64, 64]) Loss 0.02115091122686863\n",
      "Batch 62. Data shape torch.Size([16, 1, 64, 64]) Loss 0.021031180396676064\n",
      "Batch 63. Data shape torch.Size([16, 1, 64, 64]) Loss 0.02172941155731678\n",
      "Batch 64. Data shape torch.Size([16, 1, 64, 64]) Loss 0.02401670068502426\n",
      "Batch 65. Data shape torch.Size([16, 1, 64, 64]) Loss 0.015515402890741825\n",
      "Batch 66. Data shape torch.Size([16, 1, 64, 64]) Loss 0.018065154552459717\n",
      "Batch 67. Data shape torch.Size([16, 1, 64, 64]) Loss 0.023836638778448105\n",
      "Batch 68. Data shape torch.Size([16, 1, 64, 64]) Loss 0.017774149775505066\n",
      "Batch 69. Data shape torch.Size([16, 1, 64, 64]) Loss 0.0280914343893528\n",
      "Batch 70. Data shape torch.Size([16, 1, 64, 64]) Loss 0.01619294472038746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 71. Data shape torch.Size([16, 1, 64, 64]) Loss 0.02105610817670822\n",
      "Batch 72. Data shape torch.Size([16, 1, 64, 64]) Loss 0.015915749594569206\n",
      "Batch 73. Data shape torch.Size([16, 1, 64, 64]) Loss 0.008936239406466484\n",
      "Batch 74. Data shape torch.Size([16, 1, 64, 64]) Loss 0.013168173842132092\n",
      "Batch 75. Data shape torch.Size([11, 1, 64, 64]) Loss 0.013547106646001339\n",
      "Validation complete\n",
      "Run complete. Total time: 00:13:30\n"
     ]
    }
   ],
   "source": [
    "# run training\n",
    "exp.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "hippocampus_317.nii.gz Dice 0.8479, Jaccard 0.7360. 2.56% complete\n",
      "hippocampus_060.nii.gz Dice 0.8453, Jaccard 0.7321. 5.13% complete\n",
      "hippocampus_184.nii.gz Dice 0.8850, Jaccard 0.7937. 7.69% complete\n",
      "hippocampus_243.nii.gz Dice 0.6328, Jaccard 0.4629. 10.26% complete\n",
      "hippocampus_261.nii.gz Dice 0.8068, Jaccard 0.6762. 12.82% complete\n",
      "hippocampus_015.nii.gz Dice 0.7984, Jaccard 0.6645. 15.38% complete\n",
      "hippocampus_125.nii.gz Dice 0.8131, Jaccard 0.6850. 17.95% complete\n",
      "hippocampus_035.nii.gz Dice 0.8268, Jaccard 0.7047. 20.51% complete\n",
      "hippocampus_008.nii.gz Dice 0.8780, Jaccard 0.7825. 23.08% complete\n",
      "hippocampus_046.nii.gz Dice 0.8525, Jaccard 0.7429. 25.64% complete\n",
      "hippocampus_006.nii.gz Dice 0.8712, Jaccard 0.7718. 28.21% complete\n",
      "hippocampus_064.nii.gz Dice 0.8812, Jaccard 0.7876. 30.77% complete\n",
      "hippocampus_087.nii.gz Dice 0.8966, Jaccard 0.8126. 33.33% complete\n",
      "hippocampus_158.nii.gz Dice 0.8912, Jaccard 0.8038. 35.90% complete\n",
      "hippocampus_264.nii.gz Dice 0.8538, Jaccard 0.7450. 38.46% complete\n",
      "hippocampus_057.nii.gz Dice 0.8994, Jaccard 0.8172. 41.03% complete\n",
      "hippocampus_039.nii.gz Dice 0.8269, Jaccard 0.7049. 43.59% complete\n",
      "hippocampus_205.nii.gz Dice 0.8343, Jaccard 0.7157. 46.15% complete\n",
      "hippocampus_165.nii.gz Dice 0.8610, Jaccard 0.7559. 48.72% complete\n",
      "hippocampus_327.nii.gz Dice 0.8642, Jaccard 0.7608. 51.28% complete\n",
      "hippocampus_106.nii.gz Dice 0.8628, Jaccard 0.7587. 53.85% complete\n",
      "hippocampus_142.nii.gz Dice 0.9028, Jaccard 0.8229. 56.41% complete\n",
      "hippocampus_288.nii.gz Dice 0.8261, Jaccard 0.7038. 58.97% complete\n",
      "hippocampus_372.nii.gz Dice 0.8795, Jaccard 0.7850. 61.54% complete\n",
      "hippocampus_386.nii.gz Dice 0.8380, Jaccard 0.7211. 64.10% complete\n",
      "hippocampus_351.nii.gz Dice 0.8201, Jaccard 0.6950. 66.67% complete\n",
      "hippocampus_370.nii.gz Dice 0.8796, Jaccard 0.7851. 69.23% complete\n",
      "hippocampus_340.nii.gz Dice 0.8525, Jaccard 0.7428. 71.79% complete\n",
      "hippocampus_197.nii.gz Dice 0.8910, Jaccard 0.8034. 74.36% complete\n",
      "hippocampus_123.nii.gz Dice 0.8782, Jaccard 0.7828. 76.92% complete\n",
      "hippocampus_099.nii.gz Dice 0.7812, Jaccard 0.6409. 79.49% complete\n",
      "hippocampus_188.nii.gz Dice 0.8523, Jaccard 0.7425. 82.05% complete\n",
      "hippocampus_136.nii.gz Dice 0.8869, Jaccard 0.7968. 84.62% complete\n",
      "hippocampus_091.nii.gz Dice 0.8567, Jaccard 0.7494. 87.18% complete\n",
      "hippocampus_314.nii.gz Dice 0.8546, Jaccard 0.7461. 89.74% complete\n",
      "hippocampus_341.nii.gz Dice 0.8587, Jaccard 0.7523. 92.31% complete\n",
      "hippocampus_360.nii.gz Dice 0.8984, Jaccard 0.8156. 94.87% complete\n",
      "hippocampus_383.nii.gz Dice 0.8692, Jaccard 0.7687. 97.44% complete\n",
      "hippocampus_124.nii.gz Dice 0.8419, Jaccard 0.7270. 100.00% complete\n",
      "mean Dice: 0.8512, mean Jaccard 0.7435.\n",
      "\n",
      "Testing complete.\n"
     ]
    }
   ],
   "source": [
    "results_json = exp.run_test()\n",
    "\n",
    "results_json[\"config\"] = vars(c)\n",
    "\n",
    "with open(os.path.join(exp.out_dir, 'results.json'), 'w') as out_file:\n",
    "    json.dump(results_json, out_file, indent=2, separators=(',', ': '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
